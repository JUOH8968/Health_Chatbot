import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from huggingface_hub import hf_hub_download
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import shutil # shutil ëª¨ë“ˆì„ ì„í¬íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤ (ì½”ë“œ ìƒë‹¨ì— ì¶”ê°€ í•„ìš”)
import os

# ----------------------------------------------------
# --- ëª¨ë¸ ë° ê²½ë¡œ ì„¤ì • (ìˆ˜ì •ë¨) ---
# ----------------------------------------------------
# âœ… ìˆ˜ì •: ë²¡í„° DB íŒŒì¼ì´ ì €ì¥ëœ ì €ì¥ì†Œ IDë§Œ ì§€ì •í•©ë‹ˆë‹¤.
HF_REPO_ID = "ju03/Healthcare_knowledge_chatbot" 

VECTOR_DB_LOCAL_PATH = os.path.join(os.getcwd(), "vector_db")

# âœ… ìˆ˜ì •: LLM ëª¨ë¸ì€ ê³µê°œëœ KoAlpaca ëª¨ë¸ IDë¡œ ì§€ì •í•˜ì—¬ ë¡œë”© ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
LLM_MODEL_PATH = "Beomi/KoAlpaca-Polyglot-12.8B" 

EMBEDDING_MODEL_PATH = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# ----------------------------------------------------


# ğŸ”‘ ëˆ„ë½ëœ í•¨ìˆ˜ ì •ì˜: HuggingFacePipelineì˜ ì¶œë ¥ì„ ì²˜ë¦¬í•˜ëŠ” ëŒë‹¤ í•¨ìˆ˜
def extract_hf_output_text(output):
    """HuggingFacePipelineì˜ ì¶œë ¥ì´ ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¼ ê²½ìš° í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if isinstance(output, list) and output and isinstance(output[0], dict) and 'generated_text' in output[0]:
        return output[0]['generated_text']
    if isinstance(output, str):
        return output
    return str(output)


# ğŸ”‘ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ì •ì˜
def calculate_similarity(embedding_function, answer, ground_truth):
    """ë‹µë³€ê³¼ ì •ë‹µ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if not ground_truth or not answer:
        return 0.0

    try:
        embeddings_list = embedding_function.embed_documents([answer, ground_truth])
    except Exception as e:
        print(f"ì„ë² ë”© ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 0.0

    answer_embedding = np.array(embeddings_list[0]).reshape(1, -1)
    gt_embedding = np.array(embeddings_list[1]).reshape(1, -1)

    similarity = cosine_similarity(answer_embedding, gt_embedding)[0][0]
    return similarity


@st.cache_resource
def load_rag_pipeline():
    """
    RAG íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œ (LLM, Embeddings, Retriever, QA Chain)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    llm_obj = None
    retriever = None
    embeddings = None

    # status = st.status("**:gear: RAG ì±—ë´‡ êµ¬ì„± ìš”ì†Œë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...**", expanded=True)

    try:
        # 1. FAISS Vector DB íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„

        os.makedirs(VECTOR_DB_LOCAL_PATH, exist_ok=True)

        faiss_filename_in_repo = "vector_db/index.faiss"
  
        pkl_filename_in_repo = "vector_db/index.pkl"


        # index.faissì™€ index.pkl ë‹¤ìš´ë¡œë“œ
        downloaded_faiss_path = hf_hub_download(
            repo_id=HF_REPO_ID,
            filename=faiss_filename_in_repo,
            # local_dir=VECTOR_DB_LOCAL_PATH,
            local_dir_use_symlinks=False
        )
        downloaded_pkl_path = hf_hub_download(
            repo_id=HF_REPO_ID,
            filename=pkl_filename_in_repo,
            # local_dir=VECTOR_DB_LOCAL_PATH,
            local_dir_use_symlinks=False
        )
        final_faiss_path = os.path.join(VECTOR_DB_LOCAL_PATH, "index.faiss")
        final_pkl_path = os.path.join(VECTOR_DB_LOCAL_PATH, "index.pkl")
        
        # íŒŒì¼ì„ ìµœì¢… ìœ„ì¹˜ë¡œ ì´ë™
        shutil.copy(downloaded_faiss_path, final_faiss_path)
        shutil.copy(downloaded_pkl_path, final_pkl_path)

        
        # 2. Embeddings ëª¨ë¸ ë¡œë“œ
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_PATH,
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
        )
        

        # 3. Vector Store ë¡œë“œ ë° Retriever ìƒì„±
        vectorstore = FAISS.load_local(
            folder_path=VECTOR_DB_LOCAL_PATH,
            embeddings=embeddings,
            allow_dangerous_deserialization=True
        )
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        


        # 4. LLM ê°ì²´ ë¡œë“œ ë° HuggingFacePipeline ìƒì„± (ë‹µë³€ ìƒì„± ëª¨ë¸)
        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_PATH)
        model = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL_PATH,
            device_map="auto",
            low_cpu_mem_usage=True,
            torch_dtype=torch.bfloat16
        )

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.1,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id
        )
        llm_obj = HuggingFacePipeline(pipeline=pipe)


        # 5. Retrieval QA Chain êµ¬ì„±
        custom_prompt = PromptTemplate.from_template(
             """[ì§€ì‹œ]: ì œê³µëœ "ë¬¸ë§¥(Context)"ë§Œì„ ì‚¬ìš©í•˜ì—¬ "ì§ˆë¬¸(Question)"ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
             ì •ë³´ê°€ ì—†ë‹¤ë©´, "ì œê³µëœ ì •ë³´ ë‚´ì—ì„œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”.
             **ì˜¤ì§ ìµœì¢… ë‹µë³€ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì–´ë–¤ í˜•ì‹ì´ë‚˜ ì¶”ê°€ ë¬¸êµ¬ë„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

             ë¬¸ë§¥(Context): {context}

             ì§ˆë¬¸(Question): {question}

             ë‹µë³€:"""
        )

        rag_answer_chain = (
            custom_prompt
            | llm_obj
            | RunnableLambda(extract_hf_output_text)
            | StrOutputParser()
        )

        qa_chain = RunnablePassthrough.assign(
            context=(lambda x: x["question"]) | retriever,
        ).assign(
            answer=rag_answer_chain
        )
        status.success(" ìµœì¢… êµ¬ì„± ì™„ë£Œ!")
        status.update(label="**:white_check_mark: RAG ì±—ë´‡ ë¡œë“œ ì™„ë£Œ!**", state="complete", expanded=False)
        return qa_chain, retriever, embeddings

    except Exception as e:
        if status :
            status.error(f"âŒ **RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨!** ì˜¤ë¥˜ ìƒì„¸: {e}")
        else:
            st.error(f"âŒ **RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨!** ì˜¤ë¥˜ ìƒì„¸: {e}")
        return None, None, None


# --- Streamlit UI ì‹œì‘ ---

st.set_page_config(page_title="ğŸ’– í•œêµ­ì–´ í—¬ìŠ¤ì¼€ì–´ ì±—ë´‡", layout="wide")
st.title('ğŸ©º í•œêµ­ì–´ ê±´ê°• ì •ë³´ RAG ì±—ë´‡')
st.caption(f'Hugging Face ë¦¬í¬ì§€í† ë¦¬: **{HF_REPO_ID}**')

# ì±—ë´‡ êµ¬ì„± ìš”ì†Œ ë¡œë“œ
qa_chain, retriever, embeddings = load_rag_pipeline()

if qa_chain is None:
    st.warning("ì±—ë´‡ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
else:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ê±´ê°•ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # RAG ì²´ì¸ ì‹¤í–‰
                    result = qa_chain.invoke({"question": prompt})
                    
                    response = result['answer'].strip() # LLM ë‹µë³€
                    # âœ… ìˆ˜ì •: source_docsë¥¼ context í‚¤ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    source_docs = result['context']     
                    
                    # âš ï¸ ê°œì„ ëœ ë‹µë³€ í”„ë¦¬í”½ìŠ¤ ì²˜ë¦¬: 'ë‹µë³€:'ì´ í¬í•¨ëœ ê²½ìš° ì œê±°
                    response = response.strip()
                    if "ë‹µë³€:" in response:
                        response = response.split("ë‹µë³€:", 1)[1].strip()
                    elif response.lower().startswith("ë‹µë³€"):
                        response = response[len("ë‹µë³€"):].strip()

                    final_output = response
                    
                    similarity_text = ""
                    if source_docs:
                        # ê²€ìƒ‰ëœ ì²­í¬ ë‚´ìš©ì„ í•©ì³ì„œ "ì •ë‹µ ëŒ€ë¦¬" í…ìŠ¤íŠ¸ ìƒì„±
                        retrieved_text = " ".join([doc.page_content for doc in source_docs])
                        
                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                        similarity_score = calculate_similarity(embeddings, final_output, retrieved_text)
                        
                        similarity_text = f"\n\n---"
                        similarity_text += f"\n**ğŸ“ ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ê²€ìƒ‰ëœ ë¬¸í—Œ ëŒ€ë¹„ ìœ ì‚¬ë„):** `{similarity_score:.4f}`"
                        similarity_text += f" (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¬¸í—Œ ë‚´ìš©ì„ ì˜ ë°˜ì˜)"
                    else:
                        similarity_text = "\n\n---"
                        similarity_text += "\n**âš ï¸ ë‹µë³€ í’ˆì§ˆ í‰ê°€:** ê²€ìƒ‰ëœ ë¬¸í—Œì´ ì—†ì–´ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        
                    
                    st.markdown(final_output + similarity_text)
                    st.session_state.messages.append({"role": "assistant", "content": final_output + similarity_text})

                except Exception as e:
                    st.error(f"**ë‹µë³€ ìƒì„± ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ:**")
                    st.exception(e)
